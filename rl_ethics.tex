\documentclass[11pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
\usepackage{aaai}
\title{Reinforcement Learning as a Framework for Ethical Decision Making}
\author{}
\date{}                                           % Activate to display a given date or no date

% --- Note Commands ---
\usepackage{color}
\newcommand\davenote[1]{\textcolor{blue}{Dave: #1}}

\begin{document}
\maketitle

% Contributions:
% - Argument for RL as the right framework to investigate ethical learning and decision making
	% > Specific examples + motivations
		% a) Promoting ethical learning in addition to ethical decision making. (promotes adaptability and flexibility, avoids frame problem).
		% b) Promote IRL for taking into account the needs/desires of others, in a theoretically justifiable way.
		% c) Can relate the constraints of computational complexity to ethical behavior. Might be able to dig into arguments like the one Michael made in his Science article.
		% d) Completely avoids getting lost in the clouds (which is often a problem with ethics); instead, grounds ethics in a tight analytic language that lets us talk about ``what do we want the robot to do" instead of ``what is *The Good*".
% - Shortcomings of existing approaches
% - Identify open challenges 
	% > Computational (stochastic games, POMDPs)
	% > Language/Goals/Desires to reward functions.
	% > Transparency (how should an agent convey it's plan/beliefs to a human companion?)

% --- ABSTRACT ---
\begin{abstract}
% AI systems will effect humans with their decisions.
Emerging AI systems will soon be making decisions that impact the lives of humans in a significant way; It is essential, then, that these agents take into account the desires, goals, and preferences of other agents in the world while acting.
% We argue that RL should be used to inspect ethical learning & decision making.
In this work, we argue that Reinforcement Learning achieves the appropriate generality needed to theorize about an idealized ethical artificial agent, and offers the proper framework for grounding specific questions about ethical learning and decision making that can promote further scientific investigation.
% Summary of why we argue this.
Furthermore, RL can relate the often hazy territory of ethical inquiry to the grounded analytic framework of computational complexity, opening the door for speculation about the theoretical realizability of certain types of learned behaviors, given the constraints imposed by computational complexity. Consequently, we can gain insight into the way in which ethical learning must take place by confining our search for ethical learning algorithms to those that are computationally feasible. 
% Examples + critical challenges.
We review the existing approaches to ethical decision making systems, and identify critical challenges for future advancement in the area.
\end{abstract}

% --- SECTION: Introduction ---
\section{Introduction}

% Overview/motivation
Emerging AI systems will soon be making decisions that impact the lives of humans in a significant way; whether they are personal robots that are tasked with improving the daily life of a family or community,  workers in a factory setting, or virtual assistants tasked with scheduling, and improving other cosmetic aspects of an individuals life.
These systems will exist and carry out actions so as to improve the lives of the inhabitants of our planet.
It is essential, then, that these agents take into account the desires, goals, and preferences of other agents in the world while acting. \footnote{Perhaps they should focus on optimizing the goals and preferences of their owners, and no one else}.

This is critical to the decision making process of humans; our decisions do not directly benefit only our own physical being, but often benefit those around us (or avoid inflicting pain on others). We make personal sacrifices to improve the lives of others around us.

% Why RL is baller for Ethical Decision Making/Outline of document.
In this document, we propose launching an investigation into ethical decision making using the Reinforcement Learning (RL) framework.

We argue that RL achieves the appropriate generality needed to theorize about an idealized ethical artificial agent, as well as grounding specific questions about ethical decision making that can promote further scientific investigation.
Furthermore, RL can relate the often hazy territory of ethical inquiry within the grounded analytic framework of computational complexity, opening the door for speculation about the theoretical realizability of certain types of learned behaviors, given the constraints imposed by computational complexity.
In other words, we can gain insight into the way in which ethical learning must take place by confining our search for ethical learning algorithms to those that are computationally feasible. \davenote{What I had in mind here was asking about KWIK or sample complexity bounds for certain ethical ``primitives", and perhaps pitching the scaffolding paradigm, perhaps a shoutout to BPP complexity.}

% Superintelligence
\davenote{Perhaps a quick note about tying the singularity into idea in to the above (i.e. promote the pitch michael gave in Science about complexity theory being a very real constraint preventing the singularity).}
%Additionally, research attention has been focused on the feasibility of the super intelligence, and other similar considerations. Within RL, we can specifically ground these considerations in theory, and investigate the feasibility of these sorts of emergent systems.

% Posing further questions.
Using Reinforcement Learning we identify several critical challenges for future research in this area.

% Paragraph on what this document *IS*
What this document is: why we should be investigating the specifics of how an artificially intelligent agent should make ethical decisions in the reinforcement learning framework. What does it mean for a robot to be empathetic and take into consideration the needs of others? We are assuming that robotic and other types of AI systems will exist in some form or another to help humans out, and we want the decisions that these agents make to be in accordance with what we want them to do. This means coming up with a model 

% Paragraph on what this document *IS NOT*
What this document is not: is AI + warfare ethical? is technological unemployment ethical? Etc. \davenote{Purpose of this paragraph is to make sure the scope is very clear.}


% --- SECTION: Related Work ---
\section{Related Work}

\begin{itemize}
%\item AIX: aritifical general intelligence. RL is basically as general as it gets. % Note: not convinced this belongs here? I guess what I had in mind was proposing an IDEALIZED ethical decision maker, and asking what that looks like, and arriving at the conclusion that RL or some form of RL is essential.
\item Rule based systems ~\cite{briggs2015sorry}. 
\item HRI in general~\cite{scheutz2007first,tellex2011understanding}
\item Survey and warfare \davenote{I like the survey, but seems unrelated?}
\end{itemize}


% --- SECTION: Background ---
\section{Background}

We first introduce the standard Reinforcement Learning framework.


% Subsection: Reinforcement Learning
\subsection{Reinforcement Learning}

% Subsection: Inverse Reinforcement Learning
\subsection{Inverse Reinforcement Learning}

% Problem statement
Inverse reinforcement learning is...

% --- SECTION: RL for Ethical Decision Making ---
\section{RL for Ethical Decision Making}

\begin{itemize}
\item Ethical Learning. \davenote{Scaffolding, bounds on learning behavior types}. Sort of side skirting the issue of ``norms", and instead looking at, ``the agent should do what we want it to."
\item Empathy and conveying desires via IRL, interaction, HRI, transparency.
\item Adaptability (knowledge transfer, the frame problem).
\item Realizability of ethical behaviors (using complexity to inform what things are feasible, what aren't, what behaviors are learnable, what aren't, etc.).
\end{itemize}


% --- SECTION: Open Problems ---
\section{Open Problems}

Subsections:\davenote{Hope here is to pose very specific, well defined problems that other researchers could run with (hopefully folks might come away from reading this thinking ``Yeah! I want to solve that one!"}
\begin{itemize}
\item Computational (e.g. solving or formalizing POMDPs in the right way, stochastic games, multi-agent decision making, etc.)
\item How to turn desires, goals, or natural language into a reward function? e.g. James' RSS work.
\item Transparency! How should an AI agent convey its goals, objectives, beliefs, etc. to a human companion?
\end{itemize}


% --- SECTION: Conclusion ---
\section{Conclusion}


% --- BIBLIOGRAPHY ---
\bibliographystyle{aaai}
\bibliography{rl_ethics}

\end{document}
